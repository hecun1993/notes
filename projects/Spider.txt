response.selector.xpath("//title/text()")
	返回值是一个list，可以迭代的继续查询
response.selector.xpath("//title/text()").extract_first()
response.selector.css("title::text").extract_first()
	可以不加selector，直接写xpath和css就行

response.xpath('//div[@id="images"]').css('img::attr(src)').extract()
	返回值是所有图片组成的列表

response.xpath('//div[@id="images"]').css('img::attr(src)').extract_first(default="")
	为了防止报错，找不到这个属性，就加default，这样就会返回：""空

response.css("a::text").re_firse('Name\:(.*)').strip()
	用正则来匹配

xpath:
	//a/@href
	//a/text()

css:
	a::attr(href)
	a::text

scrapy默认会调用start_requests()方法，爬取我们定义的start_urls列表中的链接，然后执行默认回调函数parse()方法，并传入response参数。
	我们没有具体实现start_requests()方法，但因为我们的爬虫继承了scrapy.Spider类，所以已经帮我们实现了。
	"以parse为回调函数生成Request"

在parse函数中，可以生成item对象，交给itemPipeline处理，也可以yield新的Request。

在定义Pipeline类或者spider类的时候，可以定义一个类方法，来拿到setting中设置的变量信息

从setting中获取值，返回值是一个对象，返回的两个参数分别是mongo_uri, mongo_db
@classmethod
def from_crawler(cls, crawler):
	return cls(
		mongo_uri = crawler.settings.get('MONGO_URI')
		mongo_db = crawler.settings.get('MONGO_DB')
	)

通过获取settings中的信息来构造类，给类的全局变量进行赋值，从而构造出新的类
def __init__(self, mongo_uri, mongo_db, *args, **kwargs):
	super(ZhihuSpider, self).__init__(*args, **kwargs)
	self.mongo_uri = mongo_uri
	self.mongo_db = mongo_db

start_requests()：
	这个方法必须返回一个可迭代对象，该对象包含了spider用于爬取的第一个Request
	默认是get请求
	
	要修改为post请求：
		def start_requests(self):
			yield scrapy.Request(url='http://httpbin.org/post', method='POST', callback=self.parse_post)

		def parse_post(self, response):
			print('Hello', response.status)

make_requests_from_url(url)
	实际上，start_requests()方法，就是遍历start_urls列表中的全部url，并把这个url传递给make_requests_from_url()方法，传递的过程是用yield
	然后return Request(url, dont_filter=True) 默认的callback是parse方法

parse()
	item = ZhihuItem()
	item['text'] = quote.css()
	# 第一种：直接返回item，传递给pipeline
	yield item
	# 第二种：返回Request对象，加入调度队列中，继续爬取
	yield scrapy.Request(url=url, callback=self.parse)

logger类：输出调试信息
	def logger(self, response):
		self.logger.info(response.status)

Item Pipeline：项目管道
	数据清洗，重复检查，存储到数据库

	定义一个pipeline类，然后实现处理在spider中yield生成的item的方法
		process_item(self, item, spider)
	需要返回item / DropItem

	open_spider(self, spider)
	close_spider(self, spider)	
	from_crawler(cls, crawler): 获取settings中的设置，项目的配置信息

	使用时，在settings.py文件中，加上
	ITEM_PIPELINES = {
		'': 100,
		'': 200,
	}
	数字0~1000，代表优先级，数字越小，优先级越高。

Download Middleware：下载中间件
	改写请求，处理异常
	把requests发给downloader时
	downloader生成response，发给spiders时
		都可以通过downloader middlewares来处理

	处理request/response/异常

	可以加代理：
	class ProxyMiddleware(object):
		logger = logging.getLogger(__name__)
	
		def process_request(self, request, spider):
			self.logger.debug("Using Proxy")
			request.meta['proxy'] = 'http://123.123.23.2:9743'
			return None

		这里的return：
			None：继续下一个process_request
			request：返回到调度队列中
			response：直接进入process_response

		def process_response(self, request, response, spider):
			response.status = 201
			return response

		处理异常的方法：返回request，就是出现了异常，重新请求一次。

		def process_exception(self, request, exception, spider):
			self.logger.debug('Get Exception')
			request.meta['proxy'] = 'http://127.0.0.1:9743'
			return request

	class GoogleSpider(scrapy.Spider):
		name = "google"
		allowed_domains = ["www.google.com"]
		start_urls = ['http://www.google.com']

		def make_requests_from_url(self, url):
			return scrapy.Request(url=url, meta={'download_timeout': 10}, callback=self.parse)

		def parse(self, response):
			print(response.text)

	scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE: 查看所有的downloader_middlewares

scrapy分布式原理：
	多台主机协作的关键是，共享爬取队列，各台主机上的调度器统一从共享队列Queue中取Request

	共享队列由一台主机维护（master）
	剩下的三台从机slave，负责数据抓取，处理，存储

	redis队列
		1.key-value,结构灵活
		2.内存中的数据结构存储系统，处理速度快性能好
		3.提供队列集合等多种存储结构，方便队列维护

	怎样去重
		redis的集合，在redis集合中存储着每个request的指纹，如果指纹存在，就不把这个request添加到队列了。	

	怎样防止中断
		每台从机启动时都会先判断当前的redis request队列是否为空
		如果不为空，则取队列中的下一个request进行爬取
		如果为空，重新开始爬取，第一台从机向队列中添加request	

	scrapy-redis已经实现了这些功能
	
	使用步骤：
		改调度器
		改去重的配置
		改redis的pipeline
		redis的连接信息
			REDIS_URL = 'redis://user:pass@hostname:6379'

		(这样改，数据会存储到redis中，本机爬取，通过网络存储到master主机的redis中，所以速度会慢)(一般就会注释redispipeline)

	如何做到分布式爬取呢：
		首先把本机上的代码放到另外一台主机上，两台电脑同时执行。另外一台主机上也要有mongodb，改配置文件，可以远程访问mongodb，查看数据是否存入了。（分布式存储）

	scrapyd：
		方便部署scrapy项目，就不用git了，这样更加方便
			一是打包
			二是上传到远程服务器上
		在scrapy.cfg文件中：
			[deploy]
			url = http://123.206.65.37:6800/addversion.json
			project = zhihuuser
		命令行：scrapyd-deploy
		通过远程的接口，实现爬虫任务的启动，暂定，状态查看等

TF-IDF（Term Frequency–Inverse Document Frequency）关键词统计方法：它用以评估一字/词对于一个文件集或一个语料库中的其中一份文件的重要程度，字/词的重要性会随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

在一份给定的文件里，词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的次数。
某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。

假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是 0.03 (3/100)。一个计算文件频率 (DF) 的方法是测定有多少份文件出现过“母牛”一词，然后除以文件集里包含的文件总数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是 9.21 ( ln(10,000,000 / 1,000) )。最后的TF-IDF的分数为0.28( 0.03 * 9.21)。

因此，某个词的权重值大小与词频数不呈正比。

制作词云。

由于LDA属于概率主题模型的子类，那就先从“概率主题模型”说起：

概率主题模型（Statistical Topic Models）是一类从文本文档中提取潜在语义信息的有效方法。概率主题模型的基本原理认为文档是若干主题的混合概率分布，而每个主题又是一个关于单词的混合概率分布，可以看作是文档的一种生成模型。在概率主题的各项方法当中，潜在狄利克雷分配模型（LDA model）是最为有效的模型之一。

LDA是一种典型的无监督（也就是每段文本没有标签，我们事先不知道里面说的是啥）、基于统计学习的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。主题模型通过分析文本中的词来发现文档中的主题、主题之间的联系方式和主题的发展，通过主题模型可以使我们组织和总结无法人工标注的海量电子文档。

类似Kmeans聚类，LDA模型的主题数也需要人工来确定，笔者在尝试了多个主题数之后，确定了最终的主题数，从下面的LDA可视化图形可以看出，主题数为6时，很多主题所涵盖的关键词出现严重的重叠，而分成10个主题后，情况得到好转。

下图“打印”出这10个主题及其下辖的20个关键词，以“权重值*词汇”的累加形式呈现，各个权重值其实是该词汇在指定主题下出现的概率大小，也可以理解为该词对该主题的“贡献”程度，比如TOP0中的“孩子”前的权重系数为0.008，表明在TOP0的话题下，“孩子”被“抽中”的概率为0.008。依次类推，各个词语w在主题T下出现的概率分布称之为词分布，这个词分布也是一个多项分布。

对于上图中的主题词列表（表示与各个潜在主题最为相关的一些词语），笔者还进行了可以点击交互的可视化展示，可以看到每个主题下的关键词在该话题下及总的文本中的占比情况，从中可以看出某个词对于该主题的重要程度如何。如下图中TOP1下的关键词“摩拜”，在该主题中出现的概率最大，重要性最高，红色条柱代表它在TOP1下的比重，而蓝色条柱的是它在整个文本（88,291篇文章）中的比重。某个词对该主题重要性最显著的情况是：蓝色条柱更短、红色条柱越长，这类词更能对主题进行区隔。
