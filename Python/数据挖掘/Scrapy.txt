scrapy命令行：

scrapy startproject testproject
scrapy genspider baidu baidu.com

scrapy genspider -l
	列出所有的模版
scrapy genspider -t crawl zhihu www.zhihu.com

scrapy crawl zhihu
	执行

scrapy check
	检查当前的spider有没有语法错误

scrapy list
	项目中全部spider

scrapy fetch --nolog http://www.baidu.com
	返回页面的源代码
scrapy fetch --no-redirect --nolog --headers http://www.baidu.com
	得到响应头,禁止重定向

scrapy view http://www.baidu.com
	请求网址，然后保存到本地，在浏览器打开

scrapy shell http://www.baidu.com
	进入命令行交互模式
	有一些变量可以使用
		scrapy
		request
		response.text
		response.headers
		response.css("title::text").extract_first()

scrapy parse http://quotes.toscrape.com -callback parse
	直接输出单一的结果。方便调试。

scrapy settings --get MONGO_URL
	获取配置信息

scrapy runspider spiders.py

scrapy version -v
	查看版本信息

选择器：

scrapy shell 网址

response.selector
scrapy内置的选择器的类，可以用来提取信息

response.selector.xpath("//title/text()")
	返回值是一个list，可以迭代的继续查询
response.selector.xpath("//title/text()").extract_first()
response.selector.css("title::text").extract_first()
	可以不加selector，直接写xpath和css就行

response.xpath('//div[@id="images"]').css('img::attr(src)').extract()
	返回值是所有图片组成的列表

response.xpath('//div[@id="images"]').css('img::attr(src)').extract_first(default="")
	为了防止报错，找不到这个属性，就加default，这样就会返回：""空

response.css("a::text").re_firse('Name\:(.*)').strip()
	用正则来匹配

xpath:
	//a/@href
	//a/text()

css:
	a::attr(href)
	a::text

scrapy默认会调用start_requests()方法，爬取我们定义的start_urls列表中的链接，然后执行默认回调函数parse()方法，并传入response参数。
	我们没有具体实现start_requests()方法，但因为我们的爬虫继承了scrapy.Spider类，所以已经帮我们实现了。
	"以parse为回调函数生成Request"

在parse函数中，可以生成item对象，交给itemPipeline处理，也可以yield新的Request。

在定义Pipeline类或者spider类的时候，可以定义一个类方法，来拿到setting中设置的变量信息

# 从setting中获取值，返回值是一个对象，返回的两个参数分别是mongo_uri, mongo_db
@classmethod
def from_crawler(cls, crawler):
	return cls(
		mongo_uri = crawler.settings.get('MONGO_URI')
		mongo_db = crawler.settings.get('MONGO_DB')
	)

# 通过获取settings中的信息来构造类，给类的全局变量进行赋值，从而构造出新的类
def __init__(self, mongo_uri, mongo_db, *args, **kwargs):
	super(ZhihuSpider, self).__init__(*args, **kwargs)
	self.mongo_uri = mongo_uri
	self.mongo_db = mongo_db

start_requests()：
	这个方法必须返回一个可迭代对象，该对象包含了spider用于爬取的第一个Request
	默认是get请求
	
	要修改为post请求：
		def start_requests(self):
			yield scrapy.Request(url='http://httpbin.org/post', method='POST', callback=self.parse_post)

		def parse_post(self, response):
			print('Hello', response.status)

make_requests_from_url(url)
	实际上，start_requests()方法，就是遍历start_urls列表中的全部url，并把这个url传递给make_requests_from_url()方法，传递的过程是用yield
	然后return Request(url, dont_filter=True) 默认的callback是parse方法

parse()
	item = ZhihuItem()
	item['text'] = quote.css()
	# 第一种：直接返回item，传递给pipeline
	yield item
	# 第二种：返回Request对象，加入调度队列中，继续爬取
	yield scrapy.Request(url=url, callback=self.parse)

logger类：输出调试信息
	def logger(self, response):
		self.logger.info(response.status)

Item Pipeline：项目管道
	数据清洗，重复检查，存储到数据库

	定义一个pipeline类，然后实现处理在spider中yield生成的item的方法
		process_item(self, item, spider)
	需要返回item / DropItem

	open_spider(self, spider)
	close_spider(self, spider)	
	from_crawler(cls, crawler): 获取settings中的设置，项目的配置信息

	使用时，在settings.py文件中，加上
	ITEM_PIPELINES = {
		'': 100,
		'': 200,
	}
	数字0~1000，代表优先级，数字越小，优先级越高。

Download Middleware：下载中间件
	改写请求，处理异常
	把requests发给downloader时
	downloader生成response，发给spiders时
		都可以通过downloader middlewares来处理

	处理request/response/异常

	可以加代理：
	class ProxyMiddleware(object):
		logger = logging.getLogger(__name__)
	
		def process_request(self, request, spider):
			self.logger.debug("Using Proxy")
			request.meta['proxy'] = 'http://123.123.23.2:9743'
			return None

		这里的return：
			None：继续下一个process_request
			request：返回到调度队列中
			response：直接进入process_response

		def process_response(self, request, response, spider):
			response.status = 201
			return response

		处理异常的方法：返回request，就是出现了异常，重新请求一次。

		def process_exception(self, request, exception, spider):
			self.logger.debug('Get Exception')
			request.meta['proxy'] = 'http://127.0.0.1:9743'
			return request

	class GoogleSpider(scrapy.Spider):
		name = "google"
		allowed_domains = ["www.google.com"]
		start_urls = ['http://www.google.com']

		def make_requests_from_url(self, url):
			return scrapy.Request(url=url, meta={'download_timeout': 10}, callback=self.parse)

		def parse(self, response):
			print(response.text)

	scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE: 查看所有的downloader_middlewares

scrapy分布式原理：
	多台主机协作的关键是，共享爬取队列，各台主机上的调度器统一从共享队列Queue中取Request

	共享队列由一台主机维护（master）
	剩下的三台从机slave，负责数据抓取，处理，存储

	redis队列
		1.key-value,结构灵活
		2.内存中的数据结构存储系统，处理速度快性能好
		3.提供队列集合等多种存储结构，方便队列维护

	怎样去重
		redis的集合，在redis集合中存储着每个request的指纹，如果指纹存在，就不把这个request添加到队列了。	

	怎样防止中断
		每台从机启动时都会先判断当前的redis request队列是否为空
		如果不为空，则取队列中的下一个request进行爬取
		如果为空，重新开始爬取，第一台从机向队列中添加request	

	scrapy-redis已经实现了这些功能
	
	使用步骤：
		改调度器
		改去重的配置
		改redis的pipeline
		redis的连接信息
			REDIS_URL = 'redis://user:pass@hostname:6379'

		(这样改，数据会存储到redis中，本机爬取，通过网络存储到master主机的redis中，所以速度会慢)(一般就会注释redispipeline)

	如何做到分布式爬取呢：
		首先把本机上的代码放到另外一台主机上，两台电脑同时执行。另外一台主机上也要有mongodb，改配置文件，可以远程访问mongodb，查看数据是否存入了。（分布式存储）

	scrapyd：
		方便部署scrapy项目，就不用git了，这样更加方便
			一是打包
			二是上传到远程服务器上
		在scrapy.cfg文件中：
			[deploy]
			url = http://123.206.65.37:6800/addversion.json
			project = zhihuuser
		命令行：scrapyd-deploy
		通过远程的接口，实现爬虫任务的启动，暂定，状态查看等