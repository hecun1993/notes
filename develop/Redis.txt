redis在终止服务的时候, 不会持久化, 只有在./redis-cli shutdown的时候才会持久化.
也可以人为触发save命令, 来持久化

redis-server --port
redis-cli -p

select 1

info

flushdb 清除当前的keyspace

flushall

dbsize

save

quit

ttl 看还剩余的过期时间

expire a 10

type b 看键的类型

setex c 100 c

getset a aa 返回的是旧的值

setnx a a 如果a的键已经存在, 则不能设置, 否则可以设置


redis cookie jackson filter实现单点登录

一 基于springboot的注解

0. 添加缓存不能影响正常业务逻辑。所以要使用try catch
1. 在大流量的情况下, 缓存可以有效提高数据的读取速度.
2. 命中 失效 更新
3. 在启动类上加@EnableCaching的注解
4. 在查询方法上加@Cacheable(cacheNames = "product", key = "123") 要返回的实体类继承序列化接口
	1. 第一次访问时, 会访问数据库, 同时把数据存在redis中
	2. redis中的key: "product:123"
	3. 如果不写括号中的key, 那么其值就是方法中的参数的值
	4. 根据参数进行判断决定是否缓存: 
		@Cacheable(cacheNames = "product", key = "#sellerId", condition = "#sellerId.length() > 3")
	5. 根据结果的code判断是否要缓存: 
		@Cacheable(cacheNames = "product", key = "#sellerId", unless = "#result.getCode() != 0")
		public ResultVO list(String sellerId) {}
5. 在更新方法上加@CachePut(cacheNames = "product", key = "123")
	这样就会更新缓存
	==> @CachePut 每次都会执行方法, 然后把更新同步到redis中. 
		但是, 如果更新方法返回的是视图ModelAndView对象, 那么, 无法让ModelAndView对象实现序列化接口, 所以, @CachePut不行. ==> 需要换一种缓存同步的方式
		如果想用@CachePut和@Cacheable, 就需要查询方法和更新方法返回的数据一致.
	==> @CacheEvict(cacheNames = "product", key = "123") 只要访问这个更新方法, 就会把redis中的缓存清除掉. 从而实现缓存同步.

Redis中只能保存字符串,所以要把list等都转成json字符串，要有JsonUtils类

事务: MULTI EXEC
	Redis事务的实现需要用到 MULTI 和 EXEC 两个命令，事务开始的时候先向Redis服务器发送 MULTI 命令，然后依次发送需要在本次事务中处理的命令，最后再发送 EXEC 命令表示事务命令结束。

二 redis集群

单机版存储空间有限，需要搭建集群
	, 所有的服务器之间都互相连通.客户端只要连接其中一个服务器,
	各个节点的数据不同(如果相同,叫主备), 为了安全, 每个节点都需要加一个备份机;

	架构细节：
		(1) 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.
		(2) 节点的fail是通过集群中超过半数的节点检测失效时才生效.(所以集群中至少有三个节点, 6台服务器
		(3) Redis集群没有特定的入口。客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点，就相当于连接了所有的服务器.
		(4) 如何把所有存储的数据都分散到不同的节点上去呢?
			redis-cluster把所有的物理节点映射到[0-16383]个slot（槽）上, cluster 负责维护node<->slot<->value。
			Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点
				比如：Hello这个key,经过上面的算法算出了余数是500,然后500就是槽,找到槽对应的服务器,把key放在对应的服务器中.
			因此，搭redis集群,集群中最多有16384个节点,每个服务器代表一个槽

五种数据类型
	Redis中所有的数据都是字符串。命令不区分大小写，key是区分大小写的。Redis是单线程的。Redis中不适合保存内容大的数据。
	String: key 		(验证码，缓存，PV)
	Hash: key-fields-values		(一个key对应一个map，map中有key-value)
		hset person name jack
		hset person age 20
		hset person sex famale
		hkeys person
		hgetall person
		hvals person
	List: 有顺序可重复	(最新列表，关注列表)
	Set: 无顺序不可重复	(点赞点踩，抽奖，共同好友，已读)
	Zset: 有顺序不能重复	(排行榜)
	flushdb 清除redis数据库

Key的命令: 
	expire key second	(设置key的过期时间)
	ttl key 			(查看key的有效期)
	persist key 		(清除key的过期时间，key持久化)

redis持久化方案
	redis所有数据都存在内存中。
	RDB：快照形式，定期把内存中当前时刻的数据保存到磁盘。Redis默认支持的持久化方案。
	AOF：append only file。把所有对redis数据库操作的命令，增删改操作的命令。保存到文件中。数据库恢复时把所有的命令执行一遍即可。
	默认是RDB，如果想开启AOF，在redis.conf中搜索"aof"，appendonly no --> appendonly yes
	两种持久化方案同时开启使用aof文件来恢复数据库。

三 redis其他

1. 使用redis分布式锁:
	
	setnx key value: 如果存在key-value, 什么都不做, 否则, 增加这个key
	getset key value: 先返回之前的旧值, 然后把新值设置进去

	//以下代码为了避免死锁
	//1. 假设A线程拿到了锁, 锁已经过期了. 现在进来了B, C线程, 第一个if进不去, 后续得到的currentValue是a, 
	//2. B线程先进入if判断, 得到的oldValue为a, 然后set后oldValue成为了b, 然后内层的if判断就通过, 解锁. 也就是B线程拿到了锁
	//3. C线程再进入第二个if判断, 得到的oldValue为b, 然后set后oldValue成为了c, 但在内层的if判断中, oldValue(b)和currentValue(a)不相等, 就拿不到锁.
	//4. 从而解锁了.

	public class RedisLock {

		@Autowired
		private StringRedisTemplate redisTemplate;

		//加锁
		//key是商品id, value是过期时间
		public boolean lock(String key, String value) {
			
			if(redisTemplate.opsForValue().setIfAbsent(key, value)) {
				return true;
			}

			String currentValue = redisTemplate.opsForValue().get(key);
	        //如果锁过期（value(加锁时的时间 + 超时时间)小于当前时间，则锁过期. 可以进入这个条件判断，有可能return true，解开了死锁）
	        if (!StringUtils.isEmpty(currentValue)
	                && Long.parseLong(currentValue) < System.currentTimeMillis()) {
	            String oldValue = redisTemplate.opsForValue().getAndSet(key, value);
	            if (!StringUtils.isEmpty(oldValue)
	                    && oldValue.equals(currentValue)) {
	                return true;
	            }
	        }

	        return false;
		}

		//解锁
		public void unlock(String key, String value) {
	        try {
	            String currentValue = redisTemplate.opsForValue().get(key);
	            if (!StringUtils.isEmpty(currentValue)
	                    && currentValue.equals(value)) {
	                redisTemplate.opsForValue().getOperations().delete(key);
	            }
	        } catch (Exception e) {
	            log.error("解锁异常");
	        }
	    }
	}

2. redis的性能: 基于内存的; 单进程单线程的; KV数据库.

	2.1 单进程单线程好处
		代码更清晰，处理逻辑更简单
		不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗
		不存在多进程或者多线程导致的切换而消耗CPU
		但是, 无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善；

	2.2 Redis快的主要原因是:
		完全基于内存
		数据结构简单，对数据操作也简单
		使用多路 I/O 复用模型

同步阻塞
	用户线程通过系统调用read发起IO读操作，由用户空间转到内核空间。内核等到数据包到达后，然后将接收的数据拷贝到用户空间，完成read操作。
同步非阻塞
	同步非阻塞IO是在同步阻塞IO的基础上，将socket设置为NONBLOCK。这样做用户线程可以在发起IO请求后可以立即返回。
	由于socket是非阻塞的方式，因此用户线程发起IO请求时立即返回。但并未读取到任何数据，用户线程需要不断地发起IO请求，直到数据到达后，才真正读取到数据，继续执行。
	即用户需要不断地调用read，尝试读取socket中的数据，直到读取成功后，才继续处理接收的数据。整个IO请求的过程中，虽然用户线程每次发起IO请求后可以立即返回，但是为了等到数据，仍需要不断地轮询、重复请求，消耗了大量的CPU的资源。

I/O多路复用: 单个线程，通过记录跟踪每个I/O流(sock)的状态，来同时管理多个I/O流

	定义: 用户首先将需要进行IO操作的socket添加到select中，然后阻塞等待select系统调用返回。当数据到达时，socket被激活，select函数返回。用户线程正式发起read请求，读取数据并继续执行。
	使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

	1. 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。
	2. 多路 I/O 复用模型是利用select、poll、epoll可以同时监察多个流的 I/O 事件的能力
	3. 在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。
	4. nginx中使用了epoll，是基于事件驱动模型的，由一个或多个事件收集器来收集或者分发事件，epoll就属于事件驱动模型的事件收集器，将注册过的事件中发生的事件收集起来

## 一致性哈希算法
#### 场景一
* 假设后端集群包含三台缓存服务器，A、B、C。请求r1、r2落在A上。请求r3、r4落在B上。请求r5、r6落在C上。  
* 使用一致性哈希时，当缓存服务器B宕机时，r1/r2会仍然落在A上，r5/r6会仍然落在C上，也就是说这两台服务器上的缓存都不会失效。r3/r4会被重新分配给A或者C，并产生回源。  
* 使用其它算法，当缓存服务器B宕机时，r1/r2不再落在A上，r5/r6不再落在C上了。也就是说A、B、C上的缓存都失效了，所有的请求都要回源。

#### 场景二
* 假设有100台redis data服务器，一份数据101进来的时候，以散列公式hash(i)%100，计算所存放的服务器。假设hash(i) = 101, 那么数据被散列到标号为1的服务器。
* 然后服务器新增了一台，散列公式为hash(i)%101，这时请求访问数据101的时候，被分配至0号服务器。但是其实数据是在1号服务器的。
* 所以这个时候大量的数据失效了（访问不到了）。
* 为了解决这个问题，当新增服务器的时候：如果是持久化存储，可以让服务器集群对数据进行重新散列，进行数据迁移，然后进行恢复。但是这就意味着每次增减服务器的时候，集群就需要大量的通信，进行数据迁移，这个开销是非常大的。如果只是缓存，那么缓存就都失效了。

**关键问题在于，服务器数量变动的时候，要能够保证旧的数据能够按照老的算法，计算得到数据所在的服务器。而新的数据能够按照新的散列算法，计算出数据所在的服务器。**

---

#### 算法描述
* 1. 平衡性：
* 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。
* 2. 单调性(Monotonicity)
* 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，此时又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。

> 将对象和服务器都映射到同一个hash数值空间中，并且使用相同的hash算法。在这个环形空间中，如果沿着顺时针方向从对象的key值出发，直到遇见一个服务器，那么就将该对象存储在这个服务器上。  
> 因为对象和服务器的hash值是固定的，因此这个服务器必然是唯一和确定的。


## 哈希算法

**哈希算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。**

**如果散列一段明文而且哪怕只更改该段落的一个字母，随后的哈希都将产生不同的值。要找到散列为同一个值的两个不同的输入，在计算上是不可能的，所以数据的哈希值可以检验数据的完整性。一般用于快速查找和加密算法。**

**既然输入数据不定长，而输出的哈希值却是固定长度的，这意味着哈希值是一个有限集合，而输入数据则可以是无穷多个。那么建立一对一关系明显是不现实的。所以"碰撞"(不同的输入数据对应了相同的哈希值)是必然会发生的，所以一个成熟的哈希算法会有较好的抗冲突性。同时在实现哈希表的结构时也要考虑到哈希冲突的问题。**

**HASH算法是密码学的基础，比较常用的有MD5和SHA，最重要的两条性质，就是不可逆和无冲突。所谓不可逆，就是当你知道x的HASH值，无法求出x；所谓无冲突，就是当你知道x，无法求出一个y， 使x与y的HASH值相同。**

**这两条性质在数学上都是不成立的。因为一个函数必然可逆，且由于HASH函数的值域有限，理论上会有无穷多个不同的原始值，它们的hash值都相同。MD5和SHA做到的，是求逆和求冲突在计算上不可能，也就是正向计算很容易，而反向计算即使穷尽人类所有的计算资源都做不到。**

**记录的存储位置=f(关键字)。这里的对应关系f称为散列函数，又称为哈希（Hash函数），采用散列技术将记录存储在一块连续的存储空间中，这块连续存储空间称为散列表或哈希表（Hash table）。**

**哈希表hashtable(key，value) 就是把Key通过一个固定的算法函数即所谓的哈希函数，转换成一个整型数字，然后就将该数字对数组长度进行取余，取余结果就当作数组的下标，将value存储在以该数字为下标的数组空间里。（或者：把任意长度的输入（又叫做预映射， pre-image），通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，而不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。）**

**而当使用哈希表进行查询的时候，就是再次使用哈希函数将key转换为对应的数组下标，并定位到该空间获取value，如此一来，就可以充分利用到数组的定位性能进行数据定位**。

**哈希表有多种不同的实现方法，比如拉链法，可以理解为“链表的数组”。左边很明显是个数组，数组的每个成员包括一个指针，指向一个链表的头，当然这个链表可能为空，也可能元素很多。我们根据元素的一些特征把元素分配到不同的链表中去，也是根据这些特征，找到正确的链表，再从链表中找出这个元素。**

**Hash Table的查询速度非常的快，几乎是O(1)的时间复杂度。hash就是找到一种数据内容和数据存放地址之间的映射关系。**

哈希（Hash）是一种数据编码方式，将大尺寸数据（如一句话，一张图片，一段音乐、一个视频等）浓缩到一个数字中，从而方便地实现数据匹配·查找的功能。比如这里有一万首歌，要求按照某种方式保存好。到时候给你一首新的歌（命名为X），要求你确认新的这首歌是否在那一万首歌之内。

无疑，将一万首歌一个一个比对非常慢。但如果存在一种方式，能将一万首歌的每一首的数据浓缩到一个数字（称为哈希码）中，于是得到一万个数字，那么用同样的算法计算新的歌X的编码，看看歌X的编码是否在之前那一万个数字中，就能知道歌X是否在那一万首歌中。

将一首歌的5M字节数据浓缩到一个数字中的算法就是哈希算法。那一万首歌按照各自的编码数字从小到大排序后得到的一个表就是哈希表。

作为例子，如果要你组织那一万首歌，一个简单的哈希算法就是让歌曲所占硬盘的字节数作为哈希码。这样的话，你可以让一万首歌“按照大小排序”，然后遇到一首新的歌，只要看看新的歌的字节数是否和已有的一万首歌中的某一首的字节数相同，就知道新的歌是否在那一万首歌之内了。

对于一万首歌的规模而言，这个算法已经相当好，因为两首歌有完全相同的字节数是不大可能的。就算真有极小概率出现不同的歌有相同的哈希码，那也只有寥寥几首歌，此时再逐首比对即可。
